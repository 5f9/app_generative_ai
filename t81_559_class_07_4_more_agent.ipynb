{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_07_4_more_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 7: LangChain: Agents**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 7 Material\n",
    "\n",
    "* Part 7.1: Introduction to LangChain Agents [[Video]](https://www.youtube.com/watch?v=J5Vr___lSSs) [[Notebook]](t81_559_class_07_1_agents.ipynb)\n",
    "* Part 7.2: Understanding LangChain Agent Tools [[Video]](https://www.youtube.com/watch?v=qMquBmteYw4) [[Notebook]](t81_559_class_07_2_tools.ipynb)\n",
    "* Part 7.3: LangChain Retrival and Search Tools [[Video]](https://www.youtube.com/watch?v=NB5qGPLoBBE) [[Notebook]](t81_559_class_07_3_search_tools.ipynb)\n",
    "* **Part 7.4: Constructing LangChain Agents** [[Video]](https://www.youtube.com/watch?v=OJe5oHvrdHk) [[Notebook]](t81_559_class_07_4_more_agent.ipynb)\n",
    "* Part 7.5: Custom Agents [[Video]](https://www.youtube.com/watch?v=IsJemVYSEdc) [[Notebook]](t81_559_class_07_5_custom_agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "56693c69-259b-4a0e-8b3a-3c400869a5f8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "    os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai langchain_experimental duckduckgo-search langchainhub sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 7.4: Constructing LangChain Agents\n",
    "\n",
    "\n",
    "In this chapter, we introduce a comprehensive module that showcases the capabilities of a complete LangChain agent equipped with web search, retrieval, and memory functionalities. This agent is designed to answer questions by leveraging the most up-to-date documentation about LangChain through Retrieval-Augmented Generation (RAG), surpassing the limitations of the foundational model's knowledge. By integrating real-time web search and retrieval, the agent can access and incorporate the latest information, ensuring its responses are accurate and current. Additionally, the memory component enables the agent to retain and recall pertinent information, enhancing its ability to provide informed and contextually relevant answers. Through this module, readers will learn how to create an intelligent agent that not only understands and processes queries but also continually updates its knowledge base to stay aligned with the latest advancements in LangChain.\n",
    "\n",
    "We begin by importing the necessary libraries and modules required for our LangChain agent, including those for web search, retrieval, and memory management. Just as we have done in previous chapters, we will create an OpenAI language model (LLM) to serve as the foundation of our agent. This model will be pivotal in interpreting and generating human-like text based on the input it receives. By leveraging the OpenAI LLM, we ensure our agent has a robust and versatile language understanding capability, which will be further enhanced by integrating real-time web search and retrieval functionalities. This setup allows the agent to access and utilize the most recent and relevant information, providing comprehensive and accurate responses to user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuSzXAi7XhmT",
    "outputId": "19eb8ddf-e5d9-4356-aa97-db74365ef918"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.2,\n",
    "        n=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# llm = ChatOllama(model=\"llama3.1:8b\", base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrKUtES1xInM"
   },
   "source": [
    "\n",
    "In this section, we use Retrieval-Augmented Generation (RAG) to access the latest version of the LangChain documentation, which is more current than the data the foundational model was initially trained on. We start by creating a WebBaseLoader instance to load content from the URL https://docs.smith.langchain.com/overview, ensuring we are working with the most recent documentation. Once the content is loaded, we use RecursiveCharacterTextSplitter to break the text into smaller, manageable chunks of 1000 characters each, with an overlap of 200 characters to maintain context. These chunks are then converted into embeddings using OpenAIEmbeddings(), and we index these embeddings with FAISS (Facebook AI Similarity Search) for efficient similarity searching. Finally, we create a retriever from the FAISS index, enabling our LangChain agent to query the document embeddings and retrieve the most relevant chunks of documentation in response to user questions. This approach ensures that our agent can leverage the latest information, enhancing its ability to provide accurate and up-to-date answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kPYRfk33su71"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "# vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twXh4cQHxMqb"
   },
   "source": [
    "We now submit a simple query to the retriever to see what data it returns to answer the quest. The returned data is from the website, not yet processed by a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh9Wn2HAcFz5",
    "outputId": "ea34e71b-36bb-455d-ab7d-8d7af83f4bce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en'}, page_content='description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input[\\'postfix\\'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"how to upload a dataset\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYY0XPDrxhmp"
   },
   "source": [
    "To provide access to the web data we just downloaded, we use an agent retriever tool. This tool is created by wrapping the retriever we previously built with the VectorStoreRetrieverTool class, specifying \"Documentation\" as the name and a brief description to indicate its purpose. By integrating this agent retriever tool, our LangChain agent can effectively utilize the web data for generating responses. The ZeroShotAgent is then configured with this tool, and the agent is initialized with AgentExecutor. This setup empowers our agent to retrieve and process the most relevant information from the latest LangChain documentation, ensuring it can answer queries accurately and comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iH68G5AMcOCA"
   },
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5_lEQWJxzcj"
   },
   "source": [
    "We also demonstrate a query to the Tavily search, this will allow us to augment the downloaded data with adhoc web searches to augment both the foundation model and RAG data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tI0U0PtsIKC",
    "outputId": "e1999af9-b7ee-4515-beeb-2a7ba3436b16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.weatherapi.com/',\n",
       "  'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1729026028, 'localtime': '2024-10-15 14:00'}, 'current': {'last_updated_epoch': 1729025100, 'last_updated': '2024-10-15 13:45', 'temp_c': 15.6, 'temp_f': 60.0, 'is_day': 1, 'condition': {'text': 'Cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/119.png', 'code': 1006}, 'wind_mph': 6.7, 'wind_kph': 10.8, 'wind_degree': 252, 'wind_dir': 'WSW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 92, 'cloud': 83, 'feelslike_c': 15.5, 'feelslike_f': 59.9, 'windchill_c': 15.5, 'windchill_f': 59.9, 'heatindex_c': 15.6, 'heatindex_f': 60.0, 'dewpoint_c': 14.3, 'dewpoint_f': 57.7, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 5.1, 'gust_mph': 9.2, 'gust_kph': 14.8}}\"},\n",
       " {'url': 'https://www.timeanddate.com/weather/usa/san-francisco/hourly',\n",
       "  'content': 'Hour-by-Hour Forecast for San Francisco, California, USA. Currently: 63 ¬∞F. Partly sunny. (Weather station: San Francisco International Airport, USA). See more current weather.'},\n",
       " {'url': 'https://forecast.weather.gov/MapClick.php?lat=37.7771&lon=-122.4196',\n",
       "  'content': 'Current conditions at SAN FRANCISCO DOWNTOWN (SFOC1) Lat: 37.77056¬∞NLon: 122.42694¬∞WElev: 150.0ft. NA. 58¬∞F. 14¬∞C. ... San Francisco CA 37.77¬∞N 122.41¬∞W (Elev. 131 ft) Last Update: 11:52 pm PDT Oct 14, 2024. Forecast Valid: 1am PDT Oct 15, 2024-6pm PDT Oct 21, 2024 . Forecast Discussion . Additional Resources. Radar & Satellite Image ...'},\n",
       " {'url': 'https://www.wunderground.com/history/daily/us/ca/san-francisco/KCASANFR1762/date/2024-10-15',\n",
       "  'content': 'Current Weather for Popular Cities . San Francisco, CA warning 58 ¬∞ F Partly Cloudy; Manhattan, NY 47 ¬∞ F Clear; Schiller Park, IL (60176) warning 42 ¬∞ F Mostly Cloudy; Boston, MA warning 44 ...'},\n",
       " {'url': 'https://weatherspark.com/h/y/557/2024/Historical-Weather-during-2024-in-San-Francisco-California-United-States',\n",
       "  'content': 'San Francisco Temperature History 2024\\nHourly Temperature in 2024 in San Francisco\\nCompare San Francisco to another city:\\nCloud Cover in 2024 in San Francisco\\nDaily Precipitation in 2024 in San Francisco\\nObserved Weather in 2024 in San Francisco\\nHours of Daylight and Twilight in 2024 in San Francisco\\nSunrise & Sunset with Twilight and Daylight Saving Time in 2024 in San Francisco\\nSolar Elevation and Azimuth in 2024 in San Francisco\\nMoon Rise, Set & Phases in 2024 in San Francisco\\nHumidity Comfort Levels in 2024 in San Francisco\\nWind Speed in 2024 in San Francisco\\nHourly Wind Speed in 2024 in San Francisco\\nHourly Wind Direction in 2024 in San Francisco\\nAtmospheric Pressure in 2024 in San Francisco\\nData Sources\\n See all nearby weather stations\\nLatest Report ‚Äî 3:56 PM\\nWed, Jan 24, 2024\\xa0\\xa0\\xa0\\xa013 min ago\\xa0\\xa0\\xa0\\xa0UTC 23:56\\nCall Sign KSFO\\nTemp.\\n60.1¬∞F\\nPrecipitation\\nNo Report\\nWind\\n6.9 mph\\nCloud Cover\\nMostly Cloudy\\n1,800 ft\\nRaw: KSFO 242356Z 18006G19KT 10SM FEW015 BKN018 BKN039 16/12 A3004 RMK AO2 SLP171 T01560122 10156 20122 55001\\n While having the tremendous advantages of temporal and spatial completeness, these reconstructions: (1) are based on computer models that may have model-based errors, (2) are coarsely sampled on a 50 km grid and are therefore unable to reconstruct the local variations of many microclimates, and (3) have particular difficulty with the weather in some coastal areas, especially small islands.\\n We further caution that our travel scores are only as good as the data that underpin them, that weather conditions at any given location and time are unpredictable and variable, and that the definition of the scores reflects a particular set of preferences that may not agree with those of any particular reader.\\n 2024 Weather History in San Francisco California, United States\\nThe data for this report comes from the San Francisco International Airport.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = TavilySearchResults()\n",
    "\n",
    "search.invoke(\"what is the weather in SF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSBVXZ0QyAdQ"
   },
   "source": [
    "Next we set up the LangChain agent by integrating various tools and configuring the agent to use them effectively. We begin by defining a list of tools that includes our web search tool and the previously created retriever tool. Next, we pull a pre-defined prompt from the LangChain hub using hub.pull(\"hwchase17/openai-functions-agent\"), which provides structured messages that guide the agent's interactions. Using this prompt, we create a tool-calling agent with the create_tool_calling_agent function, passing in our language model (LLM), the list of tools, and the prompt. Finally, we initialize the agent executor with AgentExecutor, specifying the agent and tools, and enabling verbose mode for detailed logging. This setup ensures that our LangChain agent is well-equipped to leverage both the web search and retrieval functionalities to provide accurate and up-to-date responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "8dQix3CKsN7X"
   },
   "outputs": [],
   "source": [
    "tools = [search, retriever_tool]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy7KYYAPynxP"
   },
   "source": [
    "We can see how the agent responds to a simple trivial prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf6hbRpLbdj9",
    "outputId": "6a0c06f1-68b2-4e5f-c5fb-ce5948399699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'hi!'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "\"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators:\n",
      "\n",
      "score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators: [exactMatch],    metadata: {      version: \"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.\n",
      "\n",
      "how to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });\u001b[0m\u001b[32;1m\u001b[1;3mBased on the tool call response, here's an answer to your original question:\n",
      "\n",
      "\"How do I use Traceable in LangSmith?\"\n",
      "\n",
      "You can use Traceable in LangSmith by auto-tracing LLM calls using the `@traceable` decorator. This will enable automatic tracing of the function or pipeline.\n",
      "\n",
      "Here is some sample code:\n",
      "\n",
      "```python\n",
      "import openai\n",
      "from langsmith.wrappers import wrap_openai\n",
      "from langsmith import traceable\n",
      "\n",
      "# Auto-trace LLM calls in-context\n",
      "client = wrap_openai(openai.Client())\n",
      "\n",
      "@traceable # Auto-trace this function\n",
      "def pipeline(user_input: str):\n",
      "    result = client.chat.completions.create(\n",
      "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
      "        model=\"gpt-3.5-turbo\"\n",
      "    )\n",
      "    return result.choices[0].message.content\n",
      "\n",
      "pipeline(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "This will enable automatic tracing of the `pipeline` function.\n",
      "\n",
      "You can also use Traceable in TypeScript by importing it from `langsmith/traceable` and using the `@traceable` decorator on your functions or pipelines:\n",
      "\n",
      "```typescript\n",
      "import { OpenAI } from \"openai\";\n",
      "import { traceable } from \"langsmith/traceable\";\n",
      "import { wrapOpenAI } from \"langsmith/wrappers\";\n",
      "\n",
      "// Auto-trace LLM calls in-context\n",
      "const client = wrapOpenAI(new OpenAI());\n",
      "\n",
      "const pipeline = async (user_input) => {\n",
      "    const result = await client.chat.completions.create({\n",
      "        messages: [{ role: \"user\", content: user_input }],\n",
      "        model: \"gpt-3.5-turbo\",\n",
      "    });\n",
      "    return result.choices[0].message.content;\n",
      "};\n",
      "\n",
      "@traceable // Auto-trace this function\n",
      "const pipelineTraced = pipeline(\"Hello, world!\");\n",
      "```\n",
      "\n",
      "This will enable automatic tracing of the `pipeline` function.\n",
      "\n",
      "Note that you need to have Traceable installed in your LangSmith project by running `pip install langsmith-traceable` or `npm install @langsmith/traceable`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi!',\n",
       " 'output': 'Based on the tool call response, here\\'s an answer to your original question:\\n\\n\"How do I use Traceable in LangSmith?\"\\n\\nYou can use Traceable in LangSmith by auto-tracing LLM calls using the `@traceable` decorator. This will enable automatic tracing of the function or pipeline.\\n\\nHere is some sample code:\\n\\n```python\\nimport openai\\nfrom langsmith.wrappers import wrap_openai\\nfrom langsmith import traceable\\n\\n# Auto-trace LLM calls in-context\\nclient = wrap_openai(openai.Client())\\n\\n@traceable # Auto-trace this function\\ndef pipeline(user_input: str):\\n    result = client.chat.completions.create(\\n        messages=[{\"role\": \"user\", \"content\": user_input}],\\n        model=\"gpt-3.5-turbo\"\\n    )\\n    return result.choices[0].message.content\\n\\npipeline(\"Hello, world!\")\\n```\\n\\nThis will enable automatic tracing of the `pipeline` function.\\n\\nYou can also use Traceable in TypeScript by importing it from `langsmith/traceable` and using the `@traceable` decorator on your functions or pipelines:\\n\\n```typescript\\nimport { OpenAI } from \"openai\";\\nimport { traceable } from \"langsmith/traceable\";\\nimport { wrapOpenAI } from \"langsmith/wrappers\";\\n\\n// Auto-trace LLM calls in-context\\nconst client = wrapOpenAI(new OpenAI());\\n\\nconst pipeline = async (user_input) => {\\n    const result = await client.chat.completions.create({\\n        messages: [{ role: \"user\", content: user_input }],\\n        model: \"gpt-3.5-turbo\",\\n    });\\n    return result.choices[0].message.content;\\n};\\n\\n@traceable // Auto-trace this function\\nconst pipelineTraced = pipeline(\"Hello, world!\");\\n```\\n\\nThis will enable automatic tracing of the `pipeline` function.\\n\\nNote that you need to have Traceable installed in your LangSmith project by running `pip install langsmith-traceable` or `npm install @langsmith/traceable`.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ck1qk-NysM-"
   },
   "source": [
    "Next we present it with a question about langsmith, we can see the agent utilizes several tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEXPEYqMsQ8p",
    "outputId": "747d2c78-dc00-48dc-d306-a1dfa97902cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'testing features in LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators:\n",
      "\n",
      "description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[        {\"output\": \"Welcome to LangSmith\"},        {\"output\": \"Welcome to Evaluations in LangSmith\"},    ],    dataset_id=dataset.id,)# Define your evaluatordef exact_match(run, example):    return {\"score\": run.outputs[\"output\"] == example.outputs[\"output\"]}experiment_results = evaluate(    lambda input: \"Welcome \" + input['postfix'], # Your AI system goes here    data=dataset_name, # The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix=\"sample-experiment\", # The name of the experiment    metadata={      \"version\": \"1.0.0\",      \"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new\n",
      "\n",
      "= traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });    return result.choices[0].message.content;});await pipeline(\"Hello, world!\")// Out: Hello there! How can I assist you today?View a sample output trace.Learn more about tracing in the how-to guides.5. Run your first evaluation‚ÄãEvaluation requires a system to test, data to serve as test cases, and optionally evaluators to grade the results. Here we use a built-in accuracy evaluator.PythonTypeScriptfrom langsmith import Clientfrom langsmith.evaluation import evaluateclient = Client()# Define dataset: these are your test casesdataset_name = \"Sample Dataset\"dataset = client.create_dataset(dataset_name, description=\"A sample dataset in LangSmith.\")client.create_examples(    inputs=[        {\"postfix\": \"to LangSmith\"},        {\"postfix\": \"to Evaluations in LangSmith\"},    ],    outputs=[\n",
      "\n",
      "Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\u001b[0m\u001b[32;1m\u001b[1;3mBased on the output of the provided tool, it appears that LangSmith is a platform for defining and testing AI systems. It includes features such as creating a dataset, defining evaluators, and running evaluations using that dataset and evaluators.\n",
      "\n",
      "In this particular output, a sample dataset was created with the name \"Sample Dataset\". Two examples were added to the dataset, with inputs such as \"to LangSmith\" and \"to Evaluations in LangSmith\", and corresponding outputs such as \"Welcome to LangSmith\" and \"Welcome to Evaluations in LangSmith\". An evaluator was also defined, named \"exact_match\", which compares the output of a run to the expected output and returns a score of 1 if they match exactly.\n",
      "\n",
      "Finally, an evaluation was run using this dataset and evaluator, and the results were stored in a variable named \"experiment\\_results\".\n",
      "\n",
      "It's worth noting that the tool output appears to be incomplete, as it seems to be cut off before the actual results of the evaluation were provided.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'Based on the output of the provided tool, it appears that LangSmith is a platform for defining and testing AI systems. It includes features such as creating a dataset, defining evaluators, and running evaluations using that dataset and evaluators.\\n\\nIn this particular output, a sample dataset was created with the name \"Sample Dataset\". Two examples were added to the dataset, with inputs such as \"to LangSmith\" and \"to Evaluations in LangSmith\", and corresponding outputs such as \"Welcome to LangSmith\" and \"Welcome to Evaluations in LangSmith\". An evaluator was also defined, named \"exact_match\", which compares the output of a run to the expected output and returns a score of 1 if they match exactly.\\n\\nFinally, an evaluation was run using this dataset and evaluator, and the results were stored in a variable named \"experiment\\\\_results\".\\n\\nIt\\'s worth noting that the tool output appears to be incomplete, as it seems to be cut off before the actual results of the evaluation were provided.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8yXee8-y1Cf"
   },
   "source": [
    "We can also ask it a current events question, causing the agent to go to the search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CHdOOn2sRlL",
    "outputId": "c91f1743-f95c-45a1-a7c2-ade3c0f8fe84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in sf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://forecast.weather.gov/zipcity.php?inputstring=94110', 'content': 'Local Forecast Office More Local Wx 3 Day History Hourly Weather Forecast. Extended Forecast for San Francisco CA . This Afternoon. High: 73 ¬∞F. Slight Chance Drizzle. Tonight. Low: 60 ¬∞F. Chance Drizzle. Sunday. High: 72 ¬∞F. ... San Francisco CA 37.75¬∞N 122.4¬∞W (Elev. 197 ft) Last Update: 12:30 pm PDT Oct 12, 2024. Forecast Valid: 1pm PDT ...'}, {'url': 'https://www.weather.gov/mtr/?os=shmmfp&ref=app', 'content': 'NWS Forecast Office San Francisco, CA. Weather.gov > San Francisco Bay Area, CA Current Hazards. Current Outlooks; Daily Briefing; Submit Report; Detailed Hazards; Tsunami ... National Weather Service San Francisco Bay Area, CA 21 Grace Hopper Ave, Stop 5 Monterey, CA 93943-5505 (831) 656-1725 Comments? Questions? Please Contact Us. ...'}, {'url': 'https://forecast.weather.gov/zipcity.php?inputstring=San+Francisco,CA', 'content': 'Hazardous Weather Conditions. Beach Hazards Statement in effect from October 13, 03:00 PM PDT until October 16, 09:00 PM PDT; ... San Francisco CA 37.77¬∞N 122.41¬∞W (Elev. 131 ft) Last Update: 12:22 am PDT Oct 14, 2024. Forecast Valid: 1am PDT Oct 14, 2024-6pm PDT Oct 20, 2024 .'}, {'url': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629', 'content': 'Get the current and future weather conditions for San Francisco, CA, including temperature, wind, air quality, and allergy outlook. See the 10-day forecast, radar maps, and sun and moon times.'}, {'url': 'https://weather.com/weather/tenday/l/San+Francisco+CA+USCA0987:1:US', 'content': \"Comfy & Cozy\\nThat's Not What Was Expected\\nOutside\\n'No-Name Storms' In Florida\\nGifts From On High\\nWhat To Do For Wheezing\\nSurviving The Season\\nStay Safe\\nAir Quality Index\\nAir quality is considered satisfactory, and air pollution poses little or no risk.\\n Health & Activities\\nSeasonal Allergies and Pollen Count Forecast\\nNo pollen detected in your area\\nCold & Flu Forecast\\nFlu risk is low in your area\\nWe recognize our responsibility to use data and technology for good. recents\\nSpecialty Forecasts\\n10 Day Weather-San Francisco, CA\\nToday\\nMon 18 | Day\\nConsiderable cloudiness. Wed 27\\nWed 27 | Day\\nConsiderable cloudiness with occasional rain showers. Fri 29\\nFri 29 | Day\\nRain showers in the morning with scattered thunderstorms arriving in the afternoon.\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThe current weather in San Francisco is mostly cloudy, with a high of 73¬∞F and a low of 60¬∞F. There is a slight chance of drizzle this afternoon and tonight, and a chance of rain showers on Sunday. Beach Hazards Statement is also in effect from October 13 to October 16.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in sf?',\n",
       " 'output': 'The current weather in San Francisco is mostly cloudy, with a high of 73¬∞F and a low of 60¬∞F. There is a slight chance of drizzle this afternoon and tonight, and a chance of rain showers on Sunday. Beach Hazards Statement is also in effect from October 13 to October 16.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHvwUtjld_r4"
   },
   "source": [
    "## Adding Chat History/Memory\n",
    "\n",
    "To enhance our LangChain agent's capabilities, we are adding chat memory to enable it to remember previous interactions and maintain context across a conversation. This addition allows the agent to provide more coherent and contextually aware responses. We start by importing necessary modules for managing chat message histories. We then create an instance of ChatMessageHistory to store the messages exchanged during the conversation. By wrapping our existing agent executor with RunnableWithMessageHistory, we integrate the chat memory functionality. This setup ensures that each session can maintain a history of messages, facilitating smoother and more natural interactions with the agent. The code snippet below illustrates this implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9qHqnYNYZTS3"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "message_history = ChatMessageHistory()\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pr4wfiPzc33"
   },
   "source": [
    "We will now test the memory, I begin by introducing myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebpiuCU_dhsJ",
    "outputId": "a9c1414e-254f-4e81-9b91-0406226af020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello Jeff! How can I help you today? If needed, I can use tools to provide you with a more accurate and comprehensive response.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"hi! I'm Jeff\",\n",
       " 'chat_history': [HumanMessage(content=\"hi! I'm Jeff\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Based on the search results, it seems that LangSmith is an open-source AI startup that provides a unified platform for developers to track the system-level performance of their applications, model/chain performance, debug issues, and establish a broad understanding of how users are interacting with their applications. It helps teams in five core ways: debugging, providing a unified platform, tracing and evaluating complex agent prompt chains, and reducing the time required to debug and refine prompts. LangSmith's ability to track down to a single API call makes it easier to see how different components fit together. It also provides a simple UI for understanding complex workflows and logging historical data, reducing barriers to entry for those without a software background.\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Hello Jeff! How can I help you today? If needed, I can use tools to provide you with a more accurate and comprehensive response.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"hi! I'm Jeff\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"d123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNNKcKWozhEr"
   },
   "source": [
    "Next I ask it a question, we can see that it recalls who I am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4PeWelJPdmn9",
    "outputId": "8b9b6d5f-0acd-4da0-e700-d0db2287713e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help with testing by providing a unified platform for tracking system-level performance of applications, which includes model/chain performance and debugging issues. LangSmith's tracing and evaluation capabilities can assist in understanding how users interact with applications and help to debug and refine prompts more efficiently. Additionally, LangSmith's ability to track down to a single API call makes it easier to see how different components fit together during testing. It also provides a simple UI for understanding complex workflows and logging historical data, which can be useful during the testing process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can LangSmith help with testing',\n",
       " 'chat_history': [HumanMessage(content=\"hi! I'm Jeff\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Based on the search results, it seems that LangSmith is an open-source AI startup that provides a unified platform for developers to track the system-level performance of their applications, model/chain performance, debug issues, and establish a broad understanding of how users are interacting with their applications. It helps teams in five core ways: debugging, providing a unified platform, tracing and evaluating complex agent prompt chains, and reducing the time required to debug and refine prompts. LangSmith's ability to track down to a single API call makes it easier to see how different components fit together. It also provides a simple UI for understanding complex workflows and logging historical data, reducing barriers to entry for those without a software background.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"hi! I'm Jeff\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello Jeff! How can I help you today? If needed, I can use tools to provide you with a more accurate and comprehensive response.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"LangSmith can help with testing by providing a unified platform for tracking system-level performance of applications, which includes model/chain performance and debugging issues. LangSmith's tracing and evaluation capabilities can assist in understanding how users interact with applications and help to debug and refine prompts more efficiently. Additionally, LangSmith's ability to track down to a single API call makes it easier to see how different components fit together during testing. It also provides a simple UI for understanding complex workflows and logging historical data, which can be useful during the testing process.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"how can LangSmith help with testing\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"d123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whx-u4lCzm5B"
   },
   "source": [
    "We can further review and test the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Wgx_TVhdyX5",
    "outputId": "e6a84bdc-099a-4e13-d851-5c6c7efbefc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! You are Jeff, and you asked me about how LangSmith can help with testing. I provided a response explaining that LangSmith can help with testing by providing a unified platform for tracking system-level performance of applications, including model/chain performance and debugging issues. LangSmith's tracing and evaluation capabilities can assist in understanding how users interact with applications and help to debug and refine prompts more efficiently. Additionally, LangSmith's ability to track down to a single API call and provide a simple UI for understanding complex workflows and logging historical data can be useful during the testing process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who am I? What did I just ask you about?!',\n",
       " 'chat_history': [HumanMessage(content=\"hi! I'm Jeff\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Based on the search results, it seems that LangSmith is an open-source AI startup that provides a unified platform for developers to track the system-level performance of their applications, model/chain performance, debug issues, and establish a broad understanding of how users are interacting with their applications. It helps teams in five core ways: debugging, providing a unified platform, tracing and evaluating complex agent prompt chains, and reducing the time required to debug and refine prompts. LangSmith's ability to track down to a single API call makes it easier to see how different components fit together. It also provides a simple UI for understanding complex workflows and logging historical data, reducing barriers to entry for those without a software background.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"hi! I'm Jeff\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello Jeff! How can I help you today? If needed, I can use tools to provide you with a more accurate and comprehensive response.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='how can LangSmith help with testing', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"LangSmith can help with testing by providing a unified platform for tracking system-level performance of applications, which includes model/chain performance and debugging issues. LangSmith's tracing and evaluation capabilities can assist in understanding how users interact with applications and help to debug and refine prompts more efficiently. Additionally, LangSmith's ability to track down to a single API call makes it easier to see how different components fit together during testing. It also provides a simple UI for understanding complex workflows and logging historical data, which can be useful during the testing process.\", additional_kwargs={}, response_metadata={})],\n",
       " 'output': \"Hello! You are Jeff, and you asked me about how LangSmith can help with testing. I provided a response explaining that LangSmith can help with testing by providing a unified platform for tracking system-level performance of applications, including model/chain performance and debugging issues. LangSmith's tracing and evaluation capabilities can assist in understanding how users interact with applications and help to debug and refine prompts more efficiently. Additionally, LangSmith's ability to track down to a single API call and provide a simple UI for understanding complex workflows and logging historical data can be useful during the testing process.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Who am I? What did I just ask you about?!\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"d123\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
